{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"6\"> Project - Machine Learning and Computational Statistics\n",
    "    \n",
    "<font size = \"5\">Melina Moniaki - f3352321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.optimize \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import nnls\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Pavia = sio.loadmat(\"C:\\\\Users\\\\melin\\\\Desktop\\\\Data Science\\\\Machine Learning and Computational Statistics\\\\Project\\\\PaviaU_cube.mat\")\n",
    "HSI = Pavia['X'] #Pavia HSI : 300x200x103\n",
    "ends = sio.loadmat(\"C:\\\\Users\\\\melin\\\\Desktop\\\\Data Science\\\\Machine Learning and Computational Statistics\\Project\\\\PaviaU_endmembers.mat\") # Endmember's matrix: 103x9\n",
    "endmembers = ends['endmembers']\n",
    "fig = plt.figure()\n",
    "plt.plot(endmembers)\n",
    "plt.ylabel('Radiance values')\n",
    "plt.xlabel('Spectral bands')\n",
    "plt.title('9 Endmembers spectral signatures of Pavia University HSI')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endmembers.shape\n",
    "X=np.array(endmembers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X[:,1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Perform unmixing for the pixels corresponding to nonzero labels\n",
    "ground_truth= sio.loadmat(\"C:\\\\Users\\\\melin\\\\Desktop\\\\Data Science\\\\Machine Learning and Computational Statistics\\\\Project\\\\PaviaU_ground_truth.mat\")\n",
    "labels=ground_truth['y']\n",
    "fig = plt.figure()\n",
    "plt.imshow(HSI[:,:,10])\n",
    "plt.title('RGB Visualization of the 10th band of Pavia University HSI')\n",
    "plt.show()\n",
    "# For the non-negative least squares  unmixing algorithm  you can use the nnls function, see the following link:\n",
    "#https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.optimize.nnls.html#scipy.optimize.nnls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print (HSI)\n",
    "HSI.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\"> **PART 1 - SPECTRAL UNMIXING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\"> **(a) Least squares (as it was presented in the class)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we take only the pixels with nonzero class label and put them on a new array with the name Y.(it consists of 12829 pixels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In this project we take into consideration only the pixels with nonzero class label \n",
    "y=[]\n",
    "for i in range (300):\n",
    "    for j in range (200):\n",
    "        if labels[i,j] != 0:\n",
    "            y.append(HSI[i,j])\n",
    "Y=np.array(y).T           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we perform least squares to calculate the values of theta and store them in a data frame called theta_est. We also calculate the reconstruction error for this method and then and derive the 9 abundance maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform least squares unmixing\n",
    "XTX_inv = np.linalg.inv(np.dot(X.T, X))\n",
    "theta_est = np.dot(XTX_inv, X.T).dot(Y)\n",
    "\n",
    "theta_est_df=pd.DataFrame(theta_est)\n",
    "theta_est_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize a list to store reconstruction errors for each pixel\n",
    "reconstruction_errors = []\n",
    "\n",
    "# Compute reconstruction errors for each pixel\n",
    "for i in range(12829):\n",
    "    # Calculate the estimated spectral signature using the estimated abundance\n",
    "    y_est = np.dot(X, theta_est_df.iloc[:, i])\n",
    "\n",
    "    # Compute the reconstruction error for the ith pixel\n",
    "    error_i = np.square(np.linalg.norm(Y[:, i] - y_est, ord=2))  # Euclidean norm\n",
    "\n",
    "    # Append the error to the list\n",
    "    reconstruction_errors.append(error_i)\n",
    "\n",
    "# Calculate the average reconstruction error\n",
    "average_error = np.mean(reconstruction_errors)\n",
    "\n",
    "print(\"Average Reconstruction Error:\", average_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we convert the data frame with the thetas (9x12829) to  a 3D array with dimensions 300x200x9 and we create 9 different 2D arrays, each corresponding to a different endmember/material. We do this to derive the corresponding 9 abundance maps (one for each endmember/material)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a 200x300x9 array filled with zeros\n",
    "result_array = np.zeros((300, 200, 9))\n",
    "\n",
    "# Iterate over the pixels and fill in the values from the theta_est_df Data Frame\n",
    "index = 0\n",
    "for i in range(300):\n",
    "    for j in range(200):\n",
    "        if labels[i,j] != 0:  \n",
    "            result_array[i,j,:] = theta_est_df.iloc[:,index]\n",
    "            index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_arrays = []\n",
    "\n",
    "for k in range(9):\n",
    "    result_arrays.append(result_array[:, :, k])\n",
    "\n",
    "# Now result_arrays contains 9 2D arrays, each corresponding to a different k value\n",
    "df_theta = []\n",
    "for i in range(9):\n",
    "    df_a = pd.DataFrame(result_arrays[i])\n",
    "    df_theta.append(df_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    # Create the heatmap using Matplotlib's imshow function\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    mask = df_theta[i] == 0\n",
    "    \n",
    "    # Set the colormap (cmap) to 'viridis' and set_bad to white\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    cmap.set_bad(color='white')\n",
    "    \n",
    "    # Apply the mask to the data\n",
    "    im = ax.imshow(np.ma.masked_array(df_theta[i], mask))\n",
    "    \n",
    "    # Add a color bar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Set axis labels\n",
    "    ax.set_xlabel('X axis label')\n",
    "    ax.set_ylabel('Y axis label')\n",
    "    \n",
    "    # Add title\n",
    "    ax.set_title('Abundance Map {}'.format(i + 1))\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\"> **(b) Least squares imposing the sum-to-one constraint**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we perform least squares to calculate the values of theta, this time imposing the sum-to-one constraint, and store them in a data frame called theta_est_constrained. We also calculate the reconstruction error for this method and derive the 9 abundance maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the sum-to-one constraint function\n",
    "def constraint(theta):\n",
    "    return np.sum(theta) - 1.0\n",
    "\n",
    "# Define the objective function for minimization (Euclidean norm)\n",
    "def objective(theta):\n",
    "    y_est = np.dot(X, theta)\n",
    "    return np.linalg.norm(Y[:, i] - y_est, ord=2)  # Euclidean norm\n",
    "\n",
    "# Initialize an array to store the results\n",
    "theta_est_constrained = np.zeros((9, 12829))\n",
    "\n",
    "# Iterate over all pixels\n",
    "for i in range(12829):\n",
    "    # Initialize the optimization with an equal distribution\n",
    "    initial_guess = np.ones(9) / 9.0\n",
    "\n",
    "    # Define the optimization problem with the sum-to-one constraint\n",
    "    constraint_definition = {'type': 'eq', 'fun': constraint}\n",
    "    optimization_result = minimize(objective, initial_guess, constraints=constraint_definition)\n",
    "\n",
    "    # Store the optimized abundance vector\n",
    "    theta_est_constrained[:, i] = optimization_result.x\n",
    "\n",
    "# Calculate the reconstruction error\n",
    "reconstruction_error_constrained = 0\n",
    "for i in range(12829):\n",
    "    y_est_constrained = np.dot(X, theta_est_constrained[:, i])\n",
    "    reconstruction_error_constrained += np.square(np.linalg.norm(Y[:, i] - y_est_constrained, ord=2))\n",
    "\n",
    "reconstruction_error_constrained /= 12829\n",
    "\n",
    "print(\"The parameters θ1, θ2, ..., θ9 with sum-to-one constraint:\\n\", theta_est_constrained)\n",
    "print(\"The Reconstruction Error with sum-to-one constraint is:\", reconstruction_error_constrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "theta_est_constrained_df=pd.DataFrame(theta_est_constrained)\n",
    "theta_est_constrained_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a 200x300x9 array filled with zeros\n",
    "result_array_b = np.zeros((300, 200, 9))\n",
    "\n",
    "# Iterate over the pixels and fill in the values from the theta_est_constrained array\n",
    "index = 0\n",
    "for i in range(300):\n",
    "    for j in range(200):\n",
    "        if labels[i,j] != 0: \n",
    "            result_array_b[i,j,:] = theta_est_constrained_df.iloc[:,index]\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_arrays_b = []\n",
    "\n",
    "for k in range(9):\n",
    "    result_arrays_b.append(result_array_b[:, :, k])\n",
    "\n",
    "# Now result_arrays contains 9 2D arrays, each corresponding to a different k value\n",
    "df_theta_b = []\n",
    "for i in range(9):\n",
    "    df_b = pd.DataFrame(result_arrays_b[i])\n",
    "    df_theta_b.append(df_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(9):\n",
    "    # Create the heatmap using Matplotlib's imshow function\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "\n",
    "    mask = df_theta[i] == 0\n",
    "    \n",
    "    # Set the colormap (cmap) to 'viridis' and set_bad to white\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    cmap.set_bad(color='white')\n",
    "    \n",
    "    # Apply the mask to the data\n",
    "    im = ax.imshow(np.ma.masked_array(df_theta_b[i], mask))\n",
    "    \n",
    "    # Add a color bar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Set axis labels\n",
    "    ax.set_xlabel('X axis label')\n",
    "    ax.set_ylabel('Y axis label')\n",
    "    \n",
    "    # Add title\n",
    "    ax.set_title('Abundance Map {}'.format(i + 1))\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\"> **(c) Least squares imposing the non-negativity constraint on the entries of θ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we perform least squares to calculate the values of theta, this time imposing the non-negativity constraint on the entries of θ, and store them in a data frame called theta_est_non_negativity. We also calculate the reconstruction error for this method and derive the 9 abundance maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize an array to store the results\n",
    "theta_est_non_negativity = np.zeros((9, 12829))\n",
    "\n",
    "# Iterate over all pixels\n",
    "for i in range(12829):\n",
    "    # Perform non-negative least squares using nnls\n",
    "    theta_non_negativity, _ = nnls(X, Y[:, i])\n",
    "\n",
    "    # Store the non-negative least squares solution\n",
    "    theta_est_non_negativity[:, i] = theta_non_negativity\n",
    "\n",
    "# Calculate the reconstruction error\n",
    "reconstruction_error_non_negativity = 0\n",
    "for i in range(12829):\n",
    "    y_est_non_negativity = np.dot(X, theta_est_non_negativity[:, i])\n",
    "    reconstruction_error_non_negativity += np.square(np.linalg.norm(Y[:, i] - y_est_non_negativity, ord=2))\n",
    "\n",
    "reconstruction_error_non_negativity /= 12829\n",
    "\n",
    "print(\"The parameters θ1, θ2, ..., θ9 with non-negativity constraint using nnls:\\n\", theta_est_non_negativity)\n",
    "print(\"The Reconstruction Error with non-negativity constraint using nnls is:\", reconstruction_error_non_negativity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "theta_est_non_negativity_df=pd.DataFrame(theta_est_non_negativity)\n",
    "theta_est_non_negativity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a 200x300x9 array filled with zeros\n",
    "result_array_c = np.zeros((300, 200, 9))\n",
    "\n",
    "index = 0\n",
    "for i in range(300):\n",
    "    for j in range(200):\n",
    "        if labels[i,j] != 0:  \n",
    "            result_array_c[i,j,:] = theta_est_non_negativity_df.iloc[:,index]\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_arrays_c = []\n",
    "\n",
    "for k in range(9):\n",
    "    result_arrays_c.append(result_array_c[:, :, k])\n",
    "\n",
    "# Now result_arrays contains 9 2D arrays, each corresponding to a different k value\n",
    "df_theta_c = []\n",
    "for i in range(9):\n",
    "    df_c = pd.DataFrame(result_arrays_c[i])\n",
    "    df_theta_c.append(df_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(9):\n",
    "    # Create the heatmap using Matplotlib's imshow function\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "\n",
    "    mask = df_theta[i] == 0\n",
    "    \n",
    "    # Set the colormap (cmap) to 'viridis' and set_bad to white\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    cmap.set_bad(color='white')\n",
    "    \n",
    "    # Apply the mask to the data\n",
    "    im = ax.imshow(np.ma.masked_array(df_theta_c[i], mask))\n",
    "    \n",
    "    # Add a color bar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Set axis labels\n",
    "    ax.set_xlabel('X axis label')\n",
    "    ax.set_ylabel('Y axis label')\n",
    "    \n",
    "    # Add title\n",
    "    ax.set_title('Abundance Map {}'.format(i + 1))\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\"> **(d) Least squares imposing both the non-negativity and the sum-to-one constraint on the entries of θ.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we perform least squares to calculate the values of theta, this time imposing both the sum-to-one and the non-negativity constraints on the entries of θ, and store them in a data frame called theta_est_combined_constraints. We also calculate the reconstruction error for this method and derive the 9 abundance maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the sum-to-one constraint function\n",
    "def sum_to_one_constraint(theta):\n",
    "    return np.sum(theta) - 1.0\n",
    "\n",
    "# Define the objective function for minimization (Euclidean norm)\n",
    "def objective(theta):\n",
    "    y_est = np.dot(X, theta)\n",
    "    return np.linalg.norm(Y[:, i] - y_est, ord=2)  # Euclidean norm\n",
    "\n",
    "# Initialize an array to store the results\n",
    "theta_est_combined_constraints = np.zeros((9, 12829))\n",
    "\n",
    "# Iterate over all pixels\n",
    "for i in range(12829):\n",
    "    # Perform non-negative least squares using nnls as an initial guess\n",
    "    theta_non_negativity, _ = nnls(X, Y[:, i])\n",
    "\n",
    "    # Define the optimization problem with both sum-to-one and non-negativity constraints\n",
    "    constraint_definitions = [{'type': 'eq', 'fun': sum_to_one_constraint}]\n",
    "    bounds = [(0, None)] * len(theta_non_negativity)  # Non-negativity constraint\n",
    "\n",
    "    # Perform the optimization with both constraints\n",
    "    optimization_result = minimize(objective, theta_non_negativity, constraints=constraint_definitions, bounds=bounds)\n",
    "\n",
    "    # Store the optimized abundance vector\n",
    "    theta_est_combined_constraints[:, i] = optimization_result.x\n",
    "\n",
    "# Calculate the reconstruction error\n",
    "reconstruction_error_combined_constraints = 0\n",
    "for i in range(12829):\n",
    "    y_est_combined_constraints = np.dot(X, theta_est_combined_constraints[:, i])\n",
    "    reconstruction_error_combined_constraints += np.square(np.linalg.norm(Y[:, i] - y_est_combined_constraints, ord=2))\n",
    "\n",
    "reconstruction_error_combined_constraints /= 12829\n",
    "\n",
    "print(\"The parameters θ1, θ2, ..., θ9 with both constraints:\\n\", theta_est_combined_constraints)\n",
    "print(\"The Reconstruction Error with both constraints is:\", reconstruction_error_combined_constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "theta_est_combined_constraints_df=pd.DataFrame(theta_est_combined_constraints)\n",
    "theta_est_combined_constraints_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a 200x300x9 array filled with zeros\n",
    "result_array_d = np.zeros((300, 200, 9))\n",
    "\n",
    "# Iterate over the pixels and fill in the values from the theta_est array\n",
    "index = 0\n",
    "for i in range(300):\n",
    "    for j in range(200):\n",
    "        if labels[i,j] != 0: \n",
    "            result_array_d[i,j,:] = theta_est_combined_constraints_df.iloc[:,index]\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_arrays_d = []\n",
    "\n",
    "for k in range(9):\n",
    "    result_arrays_d.append(result_array_d[:, :, k])\n",
    "\n",
    "# Now result_arrays contains 9 2D arrays, each corresponding to a different k value\n",
    "df_theta_d = []\n",
    "for i in range(9):\n",
    "    df_d = pd.DataFrame(result_arrays_d[i])\n",
    "    df_theta_d.append(df_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    # Create the heatmap using Matplotlib's imshow function\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    mask = df_theta[i] == 0\n",
    "    \n",
    "    # Set the colormap (cmap) to 'viridis' and set_bad to white\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    cmap.set_bad(color='white')\n",
    "    \n",
    "    # Apply the mask to the data\n",
    "    im = ax.imshow(np.ma.masked_array(df_theta_d[i], mask))\n",
    "    \n",
    "    # Add a color bar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Set axis labels\n",
    "    ax.set_xlabel('X axis label')\n",
    "    ax.set_ylabel('Y axis label')\n",
    "    \n",
    "    # Add title\n",
    "    ax.set_title('Abundance Map {}'.format(i + 1))\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\"> **(e) LASSO, i.e., impose sparsity on θ via 𝑙1 norm minimization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did lasso for two values of alpha. alpha=0.1 and alpha = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Initialize an array to store the results\n",
    "theta_est_lasso = np.zeros((9, 12829))\n",
    "\n",
    "# Set the regularization strength\n",
    "alpha = 0.1 \n",
    "\n",
    "# Iterate over all pixels\n",
    "for i in range(12829):\n",
    "\n",
    "   # Perform LASSO regularization with increased max_iter\n",
    "    lasso = Lasso(alpha=alpha, max_iter=100000)\n",
    "    lasso.fit(X, Y[:, i])\n",
    "\n",
    "    # Store the optimized abundance vector\n",
    "    theta_est_lasso[:, i] = lasso.coef_\n",
    "\n",
    "# Calculate the reconstruction error\n",
    "reconstruction_error_lasso = 0\n",
    "\n",
    "for i in range(12829):\n",
    "    y_est_lasso = np.dot(X, theta_est_lasso[:, i])\n",
    "    reconstruction_error_lasso += np.square(np.linalg.norm(Y[:, i] - y_est_lasso, ord=2))\n",
    "\n",
    "reconstruction_error_lasso /= 12829\n",
    "\n",
    "print(\"The parameters θ1, θ2, ..., θ9 with LASSO:\\n\", theta_est_lasso)\n",
    "print(\"The Reconstruction Error with LASSO is:\", reconstruction_error_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "theta_est_lasso_df=pd.DataFrame(theta_est_lasso)\n",
    "theta_est_lasso_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a 200x300x9 array filled with zeros\n",
    "result_array_e = np.zeros((300, 200, 9))\n",
    "\n",
    "# Iterate over the pixels and fill in the values from the theta_est_lasso array\n",
    "index = 0\n",
    "for i in range(300):\n",
    "    for j in range(200):\n",
    "        if labels[i,j] != 0: \n",
    "            result_array_e[i,j,:] = theta_est_lasso_df.iloc[:,index]\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_arrays_e = []\n",
    "\n",
    "for k in range(9):\n",
    "    result_arrays_e.append(result_array_e[:, :, k])\n",
    "\n",
    "# Now result_arrays contains 9 2D arrays, each corresponding to a different k value\n",
    "df_theta_e = []\n",
    "for i in range(9):\n",
    "    df_e = pd.DataFrame(result_arrays_e[i])\n",
    "    df_theta_e.append(df_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(9):\n",
    "    # Create the heatmap using Matplotlib's imshow function\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "\n",
    "    mask = df_theta[i] == 0\n",
    "    \n",
    "    # Set the colormap (cmap) to 'viridis' and set_bad to white\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    cmap.set_bad(color='white')\n",
    "    \n",
    "    # Apply the mask to the data\n",
    "    im = ax.imshow(np.ma.masked_array(df_theta_e[i], mask))\n",
    "    \n",
    "    # Add a color bar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Set axis labels\n",
    "    ax.set_xlabel('X axis label')\n",
    "    ax.set_ylabel('Y axis label')\n",
    "    \n",
    "    # Add title\n",
    "    ax.set_title('Abundance Map {}'.format(i + 1))\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize an array to store the results\n",
    "theta_est_lasso = np.zeros((9, 12829))\n",
    "\n",
    "# Set the regularization strength\n",
    "alpha = 1 \n",
    "\n",
    "# Iterate over all pixels\n",
    "for i in range(12829):\n",
    "\n",
    "   # Perform LASSO regularization with increased max_iter\n",
    "    lasso = Lasso(alpha=alpha, max_iter=100000)\n",
    "    lasso.fit(X, Y[:, i])\n",
    "\n",
    "    # Store the optimized abundance vector\n",
    "    theta_est_lasso[:, i] = lasso.coef_\n",
    "\n",
    "# Calculate the reconstruction error\n",
    "reconstruction_error_lasso = 0\n",
    "\n",
    "for i in range(12829):\n",
    "    y_est_lasso = np.dot(X, theta_est_lasso[:, i])\n",
    "    reconstruction_error_lasso += np.square(np.linalg.norm(Y[:, i] - y_est_lasso, ord=2))\n",
    "\n",
    "reconstruction_error_lasso /= 12829\n",
    "\n",
    "print(\"The parameters θ1, θ2, ..., θ9 with LASSO:\\n\", theta_est_lasso)\n",
    "print(\"The Reconstruction Error with LASSO is:\", reconstruction_error_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "theta_est_lasso_df=pd.DataFrame(theta_est_combined_constraints)\n",
    "theta_est_lasso_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a 200x300x9 array filled with zeros\n",
    "result_array_e = np.zeros((300, 200, 9))\n",
    "\n",
    "# Iterate over the pixels and fill in the values from the theta_est array\n",
    "index = 0\n",
    "for i in range(300):\n",
    "    for j in range(200):\n",
    "        if labels[i,j] != 0:  \n",
    "            result_array_e[i,j,:] = theta_est_lasso_df.iloc[:,index]\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_arrays_e = []\n",
    "\n",
    "for k in range(9):\n",
    "    result_arrays_e.append(result_array_e[:, :, k])\n",
    "\n",
    "# Now result_arrays contains 9 2D arrays, each corresponding to a different k value\n",
    "df_theta_e = []\n",
    "for i in range(9):\n",
    "    df_e = pd.DataFrame(result_arrays_e[i])\n",
    "    df_theta_e.append(df_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(9):\n",
    "    # Create the heatmap using Matplotlib's imshow function\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    mask = df_theta[i] == 0\n",
    "    \n",
    "    # Set the colormap (cmap) to 'viridis' and set_bad to white\n",
    "    cmap = plt.get_cmap('viridis')\n",
    "    cmap.set_bad(color='white')\n",
    "    \n",
    "    # Apply the mask to the data\n",
    "    im = ax.imshow(np.ma.masked_array(df_theta_e[i], mask))\n",
    "    \n",
    "    # Add a color bar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Set axis labels\n",
    "    ax.set_xlabel('X axis label')\n",
    "    ax.set_ylabel('Y axis label')\n",
    "    \n",
    "    # Add title\n",
    "    ax.set_title('Abundance Map {}'.format(i + 1))\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For lasso with alpha = 1 we obtain better results in the abundance maps (more distinct classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"\"> **(B) Compare the results obtained from the above five methods (focusing on the abundance maps and the reconstruction error) and comment briefly on them (utilize the class information given in “PaviaU_ground_truth.mat”).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Plot the ground truth map\n",
    "labels=ground_truth['y']\n",
    "fig = plt.figure()\n",
    "plt.imshow(labels)\n",
    "plt.title('Ground truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the reference map, we can observe 9 distinct classes to which each pixel is assigned. Our goal was to determine the contribution percentage (abundance) of each pure material in the formation of a given pixel. This was achieved by calculating θi values for each pixel using different methods. In the resulting abundance maps, pixels with higher θi values appear greener, signifying a greater probability that a specific material is present in that pixel.\n",
    "\n",
    "Among the five methods employed, the \"Least Squares with both constraints\" and \"Least Squares imposing the non-negativity constraint,\" along with the \"Lasso method for a =1 \", exhibit more pronounced regions. In these methods, pixels with green coloration are clearly defined and align well with the ground truth map. However, it's worth noting that these methods also show a higher reconstruction error compared to others, with the Lasso method having the highest reconstruction error value.\n",
    "\n",
    "In the case of the first two methods, \"Least Squares\" and \"Least Squares with sum to 1 constraint,\" many regions overlap, leading to a lack of discrete regions. This contrasts with the methods mentioned earlier, where distinct regions are more apparent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\"> **PART 2 - CLASSIFICATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics as m\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " # Trainining set for classification \n",
    "Pavia_labels = sio.loadmat(\"C:\\\\Users\\\\melin\\\\Desktop\\\\Data Science\\\\Machine Learning and Computational Statistics\\\\Project\\\\classification_labels_Pavia.mat\")\n",
    "Training_Set = (np.reshape(Pavia_labels['training_set'],(200,300))).T\n",
    "Test_Set = (np.reshape(Pavia_labels['test_set'],(200,300))).T\n",
    "Operational_Set = (np.reshape(Pavia_labels['operational_set'],(200,300))).T\n",
    "fig = plt.figure()\n",
    "plt.imshow(Training_Set)\n",
    "plt.title('Labels of the pixels of the training set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df=pd.DataFrame(Training_Set)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df=pd.DataFrame(Test_Set)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Operational_Set_df = pd.DataFrame(Operational_Set)\n",
    "Operational_Set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creation of X_train and y_train\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(0,300):\n",
    "    for j in range (0,200):\n",
    "        if Training_Set[i,j] != 0 :\n",
    "            X_train.append(HSI[i,j])\n",
    "            y_train.append(Training_Set[i,j])\n",
    "X_train=np.array(X_train)\n",
    "y_train=np.array(y_train)\n",
    "X_train.shape\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creation of X_test, y_test\n",
    "X_test = []\n",
    "y_test = []\n",
    "for i in range(0,300):\n",
    "    for j in range (0,200):\n",
    "         if Test_Set[i,j] != 0 :\n",
    "            X_test.append(HSI[i,j])\n",
    "            y_test.append(Test_Set[i,j])\n",
    "X_test=np.array(X_test)\n",
    "y_test=np.array(y_test)\n",
    "# X_test\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"4\"> **Naive Bayes Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics as m\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Build a Gaussian Classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# (i) Train it based on the training set performing 10-fold cross-validation\n",
    "scores = cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy')\n",
    "mean_validation_error_NB = np.mean(1 - scores)  # error is 1 - accuracy\n",
    "std_validation_error_NB = np.std(1 - scores)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Estimated Validation Error (Mean): {mean_validation_error_NB:.2f}\")\n",
    "print(f\"Estimated Validation Error (Std): { std_validation_error_NB:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model training\n",
    "model.fit(X_train, y_train)\n",
    "# Predict Output\n",
    "y_pred_Bayes = model.predict(X_test)\n",
    "res=m.classification_report(y_test, y_pred_Bayes)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_mat_NB = confusion_matrix(y_test, y_pred_Bayes)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(confusion_mat_NB, annot=True, cmap='Blues', fmt='d', ax=ax)\n",
    "labels = ['1', '2', '3', '4', '5','6','7','8','9']\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(labels)\n",
    "ax.yaxis.set_ticklabels(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify classes that are not well separated (if any)\n",
    "poorly_separated_classes_NB = [i for i in range(confusion_mat_NB.shape[0]) if confusion_mat_NB[i, i] == 0]\n",
    "print(\"Poorly Separated Classes:\", poorly_separated_classes_NB)\n",
    "\n",
    "# Compute success rate\n",
    "success_rate_NB = np.sum(np.diag(confusion_mat_NB)) / np.sum(confusion_mat_NB)\n",
    "print(f\"Success Rate: {success_rate_NB:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"4\"> **K-nearest Neighbor Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we are going to find the best value of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "k_values = [i for i in range (1,20)]\n",
    "scores = []\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    score = cross_val_score(knn, X_train, y_train, cv=10)\n",
    "    scores.append(np.mean(score))\n",
    "    \n",
    "#Find the k value with the highest accuracy\n",
    "best_k = k_values[np.argmax(scores)]\n",
    "best_accuracy = scores[np.argmax(scores)]\n",
    "\n",
    "print(f\"The best k value is {best_k} with an accuracy of {best_accuracy:.4f}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we take the k value with the highest accuracy (k=9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import  accuracy_score\n",
    "\n",
    "# Create a k-Nearest Neighbors Classifier (with the best k-value which is 9)\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=9)  \n",
    "\n",
    "# (i) Train it based on the training set performing 10-fold cross-validation\n",
    "scores = cross_val_score(knn_classifier, X_train, y_train, cv=10, scoring='accuracy')\n",
    "mean_validation_error_knn = np.mean(1 - scores)  # error is 1 - accuracy\n",
    "std_validation_error_knn = np.std(1 - scores)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Estimated Validation Error (Mean): {mean_validation_error_knn:.2f}\")\n",
    "print(f\"Estimated Validation Error (Std): {std_validation_error_knn:.2f}\")\n",
    "\n",
    "# (ii) Train on the whole training set and evaluate on the test set\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_test = knn_classifier.predict(X_test)\n",
    "\n",
    "# Predict Output\n",
    "res=m.classification_report(y_test, y_pred_test)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "confusion_mat_KNN = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(confusion_mat_KNN, annot=True, cmap='Blues', fmt='d', ax=ax)\n",
    "labels = ['1', '2', '3', '4', '5','6','7','8','9']\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(labels)\n",
    "ax.yaxis.set_ticklabels(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify classes that are not well separated (if any)\n",
    "poorly_separated_classes_knn = [i for i in range(confusion_mat_KNN.shape[0]) if confusion_mat_KNN[i, i] == 0]\n",
    "\n",
    "# Compute success rate\n",
    "success_rate_knn = np.sum(np.diag(confusion_mat_KNN)) / np.sum(confusion_mat_KNN)\n",
    "\n",
    "# Print the results\n",
    "print(\"Poorly Separated Classes:\", poorly_separated_classes_knn)\n",
    "print(f\"Success Rate: {success_rate_knn:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"4\"> **Minimum Euclidean Distance Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First we divide the data into 10 folds\n",
    "folds_X = np.array_split(X_train, 10)\n",
    "folds_Y = np.array_split(y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implementing 10-fold cross-validation\n",
    "error_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "for i in range(10):\n",
    "    # Using the current fold as the test set and the rest as the training set\n",
    "    test_X_fold = folds_X[i]\n",
    "    test_Y_fold = folds_Y[i]\n",
    "    \n",
    "    train_X_fold = np.concatenate(folds_X[:i] + folds_X[i+1:])\n",
    "    train_Y_fold = np.concatenate(folds_Y[:i] + folds_Y[i+1:])\n",
    "    \n",
    "    # Calculating the mean of each class in the training set\n",
    "    class_means = {}\n",
    "    for label in np.unique(train_Y_fold):\n",
    "        class_samples = train_X_fold[train_Y_fold == label]\n",
    "        class_means[label] = np.mean(class_samples, axis=0)\n",
    "        \n",
    "    # Classifying the test set using the minimum Euclidean distance\n",
    "    predictions = []\n",
    "    for sample in test_X_fold:\n",
    "        distances = []\n",
    "        for label, mean in class_means.items():\n",
    "            distance = np.linalg.norm(sample - mean)\n",
    "            distances.append((distance, label))\n",
    "        prediction = min(distances)[1]\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "    # Calculating misclassification error and accuracy on the test set\n",
    "    error = np.mean(predictions != test_Y_fold)\n",
    "    error_list.append(error)\n",
    "    \n",
    "    accuracy = np.mean(predictions == test_Y_fold)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "# print results\n",
    "print(\"Average Accuracy: %0.2f with standard deviation: %0.2f\" % (np.mean(accuracy_list), np.std(accuracy_list)))\n",
    "print(\"Average Error: %0.2f with standard deviation: %0.2f\" % (np.mean(error_list), np.std(error_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculating the mean of each class in the training set\n",
    "class_means_dict = {}\n",
    "\n",
    "for index, class_label in enumerate(np.unique(y_train)):\n",
    "    class_samples_train = X_train[y_train == class_label]\n",
    "    class_means_dict[class_label] = np.mean(class_samples_train, axis=0)\n",
    "\n",
    "# For each test sample, compute its Euclidean distance to each class mean\n",
    "predictions_list = []\n",
    "\n",
    "for sample_index in range(len(X_test)):\n",
    "    distances_list = []\n",
    "    \n",
    "    for class_label, mean_vector in class_means_dict.items():\n",
    "        distance_to_mean = np.linalg.norm(X_test[sample_index] - mean_vector)\n",
    "        distances_list.append((distance_to_mean, class_label))\n",
    "\n",
    "    # Assign the test sample to the class with the smallest distance\n",
    "    predicted_class = min(distances_list)[1]\n",
    "    predictions_list.append(predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res=m.classification_report(y_test, predictions_list)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "confusion_mat_EMD = confusion_matrix(y_test, predictions_list)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(confusion_mat_EMD, annot=True, cmap='Blues', fmt='d', ax=ax)\n",
    "labels = ['1', '2', '3', '4', '5','6','7','8','9']\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(labels)\n",
    "ax.yaxis.set_ticklabels(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (i) Identify classes that are not well separated (if any)\n",
    "poorly_separated_classes_EMD = [i for i in range(confusion_mat_EMD.shape[0]) if confusion_mat_EMD[i, i] == 0]\n",
    "\n",
    "# (ii) Compute success rate \n",
    "success_rate_EMD = np.sum(np.diag(confusion_mat_EMD)) / np.sum(confusion_mat_EMD)\n",
    "\n",
    "print(\"Poorly Separated Classes (EMD):\", poorly_separated_classes_EMD)\n",
    "print(f\"Success Rate (EMD): {success_rate_EMD:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = '4'> **Bayesian classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create a Linear Discriminant Analysis Classifier\n",
    "lda_classifier = LinearDiscriminantAnalysis()\n",
    "\n",
    "# (i) Train it based on the training set performing 10-fold cross-validation\n",
    "k = 10\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Use cross_val_score to perform k-fold cross-validation and obtain accuracy scores\n",
    "scores = cross_val_score(lda_classifier, X_train, y_train, cv=kf, scoring='accuracy')\n",
    "mean_validation_error_lda = 1 - np.mean(scores)  # error is 1 - accuracy\n",
    "std_validation_error_lda = np.std(1 - scores)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Estimated Validation Error (Mean): {mean_validation_error_lda:.2f}\")\n",
    "print(f\"Estimated Validation Error (Std): {std_validation_error_lda:.2f}\")\n",
    "\n",
    "# (ii) Train on the whole training set and evaluate on the test set\n",
    "lda_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_test_lda = lda_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier on the test set\n",
    "print(\"Classification Report on Test Set:\")\n",
    "print(classification_report(y_test, y_pred_test_lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the confusion matrix\n",
    "confusion_mat_lda = confusion_matrix(y_test, y_pred_test_lda)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(confusion_mat_lda, annot=True, cmap='Blues', fmt='d', ax=ax)\n",
    "labels = ['1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "ax.set_title('Confusion Matrix (LDA)')\n",
    "ax.xaxis.set_ticklabels(labels)\n",
    "ax.yaxis.set_ticklabels(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (i) Identify classes that are not well separated (if any) for LDA\n",
    "poorly_separated_classes_lda = [i for i in range(confusion_mat_lda.shape[0]) if confusion_mat_lda[i, i] == 0]\n",
    "\n",
    "# (ii) Compute success rate for LDA\n",
    "success_rate_lda = np.sum(np.diag(confusion_mat_lda)) / np.sum(confusion_mat_lda)\n",
    "\n",
    "# (iii) Print the results for LDA\n",
    "print(\"Poorly Separated Classes (LDA):\", poorly_separated_classes_lda)\n",
    "print(f\"Success Rate (LDA): {success_rate_lda:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"4\">Compare the results of the classifiers and comment on them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the success rate of each classifier we can deduce that the best classifier is the KNN classifier (89% success rate) , followed by the Bayes classifier (88%) , then the Naive Bayes classifier (66%) and lastly the minimum euclidean distance classifier (56%).\n",
    "From the confusion matrices of our two best classifiers (knn and Bayes classifiers) we can see that there is a confusion between classes 3 and 8. \n",
    "Based on the Bayes classifier 100 labels are missclasified as 8 (true label = 3) from total of 536. So the missclasification error for class 3 is 18.66%. Also, 74 labels are misslasified as 3 (true label=8) from total of 461. So the missclasification error for class 8 is 16%.\n",
    "Based on knn classifier, 72 labels are missclasified as 8 from total of 536(true label=3). So the misclassification error for class 3 is 13.43%. Also, 85 labels are misclassified as 3 (true label= 8) from total of 461. So the missclasification error for class 8 is : 18.44 %.\n",
    "Furthermore, there are instances of confusion among additional classes, although the degree of confusion is not as pronounced as that observed between classes 3 and 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"5\"> **PART 3 - COMBINATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"4\"> Comment briefly on the possible correlation of the results obtained from the spectral unmixing procedure with those obtained from classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a noticeable correlation between the outcomes derived from the spectral unmixing procedure and those obtained through classification. Specifically, employing the best unmixing methods, such as Least Squares with both constraints and Non-Negativity Constraints, reveals distinctly separated classes, indicated by a prominent green coloration. Similarly, our top-performing classifiers, namely k-Nearest Neighbors (knn) and Bayes, exhibit nearly diagonal confusion matrices, implying accurate assignment of the majority of pixels to their true classes. In spectral unmixing, higher values of θi signify a greater likelihood of assigning a specific pixel to class i, mirroring the approach taken in the classification method. Both methods demonstrate effective performance. \n",
    "Additionally, our classification method reveals confusion between classes 3 and 8, a phenomenon corroborated by the abundance maps. Notably, some pixels with true label 3 are colored green in abundance map 8, and vice versa, underscoring the intricacies of class distinctions in both methodologies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
